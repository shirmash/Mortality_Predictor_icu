{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "6654d3516ba4a90d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from itertools import product, combinations\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_csv('/Users/ormeiri/Desktop/predictive_models_git/data/chapter 4/data/multiple_imputation.csv')",
   "id": "7d64d03032c040ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_variable = 'hospital_death'\n",
    "X = df.drop(columns=[target_variable])\n",
    "y = df[target_variable]"
   ],
   "id": "ddcbbf0e45526c41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_data(X, categorical_cols=None, is_training=True, scaler=None, dummy_cols=None):\n",
    "    \"\"\"\n",
    "    Preprocess data by creating dummy variables and scaling\n",
    "    \"\"\"\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    # Identify categorical columns if not provided\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = X_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Create dummy variables for categorical columns\n",
    "    if categorical_cols:\n",
    "        if is_training:\n",
    "            X_processed = pd.get_dummies(X_processed, columns=categorical_cols, drop_first=True)\n",
    "            dummy_cols = X_processed.columns.tolist()\n",
    "        else:\n",
    "            # For test data, ensure same columns as training\n",
    "            X_processed = pd.get_dummies(X_processed, columns=categorical_cols, drop_first=True)\n",
    "            # Align columns with training data\n",
    "            for col in dummy_cols:\n",
    "                if col not in X_processed.columns:\n",
    "                    X_processed[col] = 0\n",
    "            X_processed = X_processed[dummy_cols]\n",
    "\n",
    "    # Scale numerical features for LogReg and KNN\n",
    "    if is_training:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(X_processed),\n",
    "            columns=X_processed.columns,\n",
    "            index=X_processed.index\n",
    "        )\n",
    "    else:\n",
    "        X_scaled = pd.DataFrame(\n",
    "            scaler.transform(X_processed),\n",
    "            columns=X_processed.columns,\n",
    "            index=X_processed.index\n",
    "        )\n",
    "\n",
    "    return X_scaled, scaler, dummy_cols"
   ],
   "id": "177856325fca96a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()"
   ],
   "id": "4bf818f9878678dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                        random_state=42, stratify=y)"
   ],
   "id": "1282795bfc21e1ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # Preprocess data for LogReg and KNN (dummy variables + scaling)\n",
    "X_train_scaled, scaler, dummy_cols = preprocess_data(X_train, categorical_cols, is_training=True)\n",
    "X_test_scaled, _, _ = preprocess_data(X_test, categorical_cols, is_training=False,\n",
    "                                    scaler=scaler, dummy_cols=dummy_cols)\n",
    "\n",
    "# Preprocess data for XGBoost (dummy variables only, no scaling)\n",
    "if categorical_cols:\n",
    "    X_train_xgb = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "    X_test_xgb = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "    # Align columns\n",
    "    for col in X_train_xgb.columns:\n",
    "        if col not in X_test_xgb.columns:\n",
    "            X_test_xgb[col] = 0\n",
    "    X_test_xgb = X_test_xgb[X_train_xgb.columns]\n",
    "else:\n",
    "    X_train_xgb = X_train\n",
    "    X_test_xgb = X_test"
   ],
   "id": "ba0c46fce0dfd9a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "knn_path = \"/Users/ormeiri/Desktop/predictive_models_git/models/final_models/best_knn.pkl\"\n",
    "with open(knn_path, 'rb') as f:\n",
    "    knn = pickle.load(f)\n",
    "lr_path = \"/Users/ormeiri/Desktop/predictive_models_git/models/final_models/best_logistic_regression.pkl\"\n",
    "with open(lr_path, 'rb') as f:\n",
    "    lr = pickle.load(f)\n",
    "xgboost_path = \"/Users/ormeiri/Desktop/predictive_models_git/models/final_models/best_xgboost.pkl\"\n",
    "with open(xgboost_path, 'rb') as f:\n",
    "    xgboost = pickle.load(f)"
   ],
   "id": "fa351ff5cd438263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fairness",
   "id": "d8db2144af5a85b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_subgroups(query_lists):\n",
    "    \"\"\"\n",
    "    Generates all possible subgroups (tuples) formed by combining elements from the input lists.\n",
    "\n",
    "    Args:\n",
    "        query_lists (list of lists): A list of lists, where each sublist represents a set of elements to combine.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple represents a subgroup formed by combining elements across the input lists.\n",
    "    \"\"\"\n",
    "    subgroups = []\n",
    "    for list1, list2 in combinations(query_lists, 2):\n",
    "        subgroups.extend(product(list1, list2))\n",
    "    for list1, list2, list3 in combinations(query_lists, 3):\n",
    "        subgroups.extend(product(list1, list2, list3))\n",
    "    for list1, list2, list3, list4 in combinations(query_lists, 4):\n",
    "        subgroups.extend(product(list1, list2, list3, list4))\n",
    "    return subgroups"
   ],
   "id": "d5b401a79c364ae0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_empty_subgroups(subpopulation_queries, X):\n",
    "  \"\"\"\n",
    "    Identifies subgroups within a dataset that have no corresponding data points (i.e., empty subgroups).\n",
    "\n",
    "    Args:\n",
    "        subpopulation_queries (list): A list of queries defining the subgroups. Each query can be a tuple of conditions or a single condition string.\n",
    "        X (pandas.DataFrame): The DataFrame containing the data to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of queries that resulted in empty subgroups.\n",
    "  \"\"\"\n",
    "  empty_queries = []\n",
    "  for query in subpopulation_queries:\n",
    "    S = X.query(\" and \".join(query) if isinstance(query, tuple) else query).index\n",
    "\n",
    "    if len(S) == 0:\n",
    "      empty_queries.append(query)\n",
    "\n",
    "  return empty_queries"
   ],
   "id": "45e77b509f04847e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gender_queries = ['gender_M == 1', 'gender_M == 0',]\n",
    "age_queries = [f'age >= {a} and age < {a+10}' for a in range(10, 90, 10)]\n",
    "# List of ethnicity columns that, if all are 0, imply 'African American'\n",
    "other_ethnicities = ['Caucasian', 'Native American', 'Other/Unknown', 'Asian']\n",
    "\n",
    "# Queries for each ethnicity\n",
    "ethnicity_queries = [f'`ethnicity_{e}` == 1' for e in other_ethnicities]\n",
    "\n",
    "# African American: if all others are 0\n",
    "african_american_query = ' and '.join([f'`ethnicity_{e}` == 0' for e in other_ethnicities])\n",
    "ethnicity_queries += [african_american_query]\n",
    "\n",
    "all_query_lists = [gender_queries, age_queries, ethnicity_queries]\n",
    "all_subgroups = generate_subgroups(all_query_lists)\n",
    "\n",
    "subpopulation_queries = [\n",
    "  *gender_queries,\n",
    "  *age_queries,\n",
    "  *ethnicity_queries,\n",
    "  *all_subgroups\n",
    "]\n",
    "\n",
    "print(f'Total subgroups = {len(subpopulation_queries)}')\n",
    "\n",
    "empty_subgroups = find_empty_subgroups(subpopulation_queries, X_test_xgb) # 6 minutes to run (pickled for later use)\n",
    "final_subgroup_queries = set(subpopulation_queries) - set(empty_subgroups)\n",
    "len(final_subgroup_queries)\n",
    "\n",
    "sorted_queries = sorted(final_subgroup_queries, key=lambda x: 0 if isinstance(x, str) else len(x))\n",
    "print(f'Valid subgroups = {len(sorted_queries)}')"
   ],
   "id": "291e4dc09df546f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ClibratedPredictor:\n",
    "  def __init__(self, model, subgroups, y_test):\n",
    "    self.model = model\n",
    "    self.subgroups = subgroups\n",
    "    self.y_test = y_test\n",
    "    self.proba = None\n",
    "\n",
    "  def predict(self, X, y=None):\n",
    "    if self.proba is None:\n",
    "      self.predict_proba(X, y)\n",
    "\n",
    "    p = self.proba[:, 1]\n",
    "    preds = np.where(p >= 0.2, 1, 0)\n",
    "\n",
    "    return preds\n",
    "\n",
    "  def predict_proba(self, X, y=None):\n",
    "    return self.multi_calibrate_predictor(X, y, self.subgroups)\n",
    "\n",
    "  def multi_calibrate_predictor(self, X, y, subpopulation_queries, alpha=1e-3, max_iter=1):\n",
    "    \"\"\"\n",
    "      Perform multi-calibration to ensure fairness across subgroups.\n",
    "\n",
    "      :param predictor: A trained classifier with a predict_proba method.\n",
    "      :param X: DataFrame containing the input features.\n",
    "      :param y: Series containing the true labels.\n",
    "      :param subpopulation_queries: List of strings representing the query to define each subgroup.\n",
    "      :param alpha: The violation parameter.\n",
    "      :param max_iter: Maximum number of iterations for the calibration process.\n",
    "      :return: Calibrated probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    y = pd.Series(self.y_test, index=X.index)\n",
    "    p = self.model.predict_proba(X)[:, 1]\n",
    "    calibrated_p = pd.Series(p.copy(), index=X.index)\n",
    "    done = False\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "      done = True\n",
    "      for query in subpopulation_queries:\n",
    "        S = X.query(\" and \".join(query) if isinstance(query, tuple) else query).index\n",
    "\n",
    "        delta_S = (y.loc[S] - calibrated_p.loc[S]).mean()\n",
    "        if abs(delta_S) > alpha:\n",
    "          calibrated_p.loc[S] += delta_S\n",
    "          done = False\n",
    "\n",
    "      if done: print(\"calibrated\"); break\n",
    "\n",
    "    calibrated_p[calibrated_p < 0] = 0\n",
    "    calibrated_p[calibrated_p > 1] = 1\n",
    "\n",
    "    calibrated_p = calibrated_p.values  # Not P-values XD\n",
    "    zero_class = 1 - calibrated_p\n",
    "    calibrated_p = np.concatenate([zero_class[:,np.newaxis], calibrated_p[:,np.newaxis]], axis=1)\n",
    "\n",
    "    self.proba = calibrated_p\n",
    "\n",
    "    return calibrated_p"
   ],
   "id": "451f1171d50d971b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compare_performance_measures(X_test, y_test, model, subgroup_conds, model_name):\n",
    "    \"\"\"\n",
    "    Function to compare all performance measures across subgroups for a single model.\n",
    "\n",
    "    :param X_test: DataFrame containing the test features.\n",
    "    :param y_test: Series containing the test labels.\n",
    "    :param model: A single model to evaluate.\n",
    "    :param subgroup_conds: List of conditions defining the subgroups.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics_data = []\n",
    "    metrics_types = ['AUC', 'Sensitivity', 'PPV', 'Specificity', 'NPV']\n",
    "\n",
    "    # Iterate over each subgroup condition\n",
    "    for cond, label in subgroup_conds.items():\n",
    "        query_str = \" and \".join(cond) if isinstance(cond, tuple) else cond\n",
    "        sub_df = X_test.query(query_str)\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "        true_labels = y_test.loc[sub_df.index]\n",
    "\n",
    "        predictions = model.predict(sub_df)\n",
    "        pred_proba = model.predict_proba(sub_df)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
    "\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics_data.append({\n",
    "            'Subgroup': label,\n",
    "            'Metric': 'AUC',\n",
    "            'Value': roc_auc_score(true_labels, pred_proba) if hasattr(model, \"predict_proba\") and len(true_labels.value_counts()) > 1 else None\n",
    "\n",
    "        })\n",
    "        metrics_data.append({'Subgroup': label, 'Metric': 'Sensitivity', 'Value': tp / (tp + fn)})\n",
    "        metrics_data.append({'Subgroup': label, 'Metric': 'PPV', 'Value': tp / (tp + fp)})\n",
    "        metrics_data.append({'Subgroup': label, 'Metric': 'Specificity', 'Value': tn / (tn + fp)})\n",
    "        metrics_data.append({'Subgroup': label, 'Metric': 'NPV', 'Value': tn / (tn + fn) if (tn + fn) != 0 else None})\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_data)\n",
    "\n",
    "    # Plotting each metric for each subgroup using Plotly Express\n",
    "\n",
    "    color_sequence = ['red', 'green', 'blue', 'orange', 'purple']\n",
    "    fig = px.bar(df_metrics, x='Subgroup', y='Value', color='Metric', barmode='group', title=f\"Performance Metrics by Subgroup with {model_name}\", color_discrete_sequence=color_sequence)\n",
    "    fig.update_xaxes(tickfont=dict(size=16))\n",
    "\n",
    "    fig.update_layout(xaxis_title='Subgroup', yaxis_title='Metric Value')\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def model_comparison(model, model_name):\n",
    "  print(\"Let's compare basic models\")\n",
    "\n",
    "  males_vs_females = {\n",
    "    'age > 0' : 'All data',\n",
    "    'gender_M == 1': 'Males',\n",
    "    'gender_F == 1': 'Females',\n",
    "  }\n",
    "  compare_performance_measures(X_test, y_test, model, males_vs_females, model_name)\n",
    "\n",
    "  age_subgroups = {\n",
    "    'age > 0' : 'All data',\n",
    "    'age < 30': 'Younger than 30',\n",
    "    'age >= 30 and age < 40': 'Aged 30 to 39',\n",
    "    'age >= 40 and age < 50': 'Aged 40 to 49',\n",
    "    'age >= 50 and age < 60': 'Aged 50 to 59',\n",
    "    'age >= 60 and age < 70': 'Aged 60 to 69',\n",
    "    'age >= 70 and age < 80': 'Aged 70 to 79',\n",
    "    'age >= 80': 'Aged 80 and older',\n",
    "  }\n",
    "  compare_performance_measures(X_test, y_test, xgboost, age_subgroups, model_name)\n",
    "\n",
    "  ethnicity_subgroups = {\n",
    "      'age > 0' : 'All data',\n",
    "      'ethnicity_Caucasian == 1': 'Caucasian',\n",
    "      '`ethnicity_Native American` == 1': 'Native American',\n",
    "      '`ethnicity_Other/Unknown` == 1': 'Other/Unknown Ethnicity',\n",
    "      '`ethnicity_African American` == 1': 'African American',\n",
    "      'ethnicity_Asian == 1': 'Asian',\n",
    "  }\n",
    "  compare_performance_measures(X_test, y_test, xgboost, ethnicity_subgroups, model_name)\n",
    "\n",
    "  basic_subgroups_dict = {\n",
    "      'age > 0' : 'All data',\n",
    "      '`hospital_id_30.0` == 1': 'Hospital #30 Patients',\n",
    "      '`hospital_id_70.0` == 1': 'Hospital #70 Patients',\n",
    "      '`hospital_id_100.0` == 1': 'Hospital #100 Patients',\n",
    "      '`hospital_id_118.0` == 1': 'Hospital #118 Patients',\n",
    "  }\n",
    "  compare_performance_measures(X_test, y_test, xgboost, basic_subgroups_dict, model_name)\n",
    "\n",
    "  print(\"Let's go deeper\")\n",
    "\n",
    "  intersecting_2_subgroups_dict = {\n",
    "    'age > 0' : 'All data',\n",
    "    ('age >= 80', 'gender_F == 1'): 'Females Aged 80 and older',\n",
    "    ('ethnicity_Caucasian == 1', 'gender_M == 1'): 'Caucasian Males',\n",
    "    ('ethnicity_Asian == 1', 'gender_F == 1'): 'Asian Females',\n",
    "    ('`hospital_id_19.0` == 1', '`ethnicity_African American` == 1'): 'Hospital #19 African American Patients',\n",
    "    ('gender_F == 1', 'ethnicity_Caucasian == 1'): 'Caucasian Females',\n",
    "    ('`hospital_id_188.0` == 1', 'gender_F == 1'): 'Hospital #188 Female Patients',\n",
    "  }\n",
    "\n",
    "  compare_performance_measures(X_test, y_test, xgboost, intersecting_2_subgroups_dict, model_name)\n",
    "\n",
    "  print(\"Even deeper\")\n",
    "  intersecting_3_subgroups_dict = {\n",
    "    'age > 0' : 'All data',\n",
    "    ('gender_M == 1', 'age < 30', '`hospital_id_30.0` == 1'): 'Young Males at Hospital #30',\n",
    "    ('gender_F == 1', 'ethnicity_Caucasian == 1', '`hospital_id_70.0` == 1'): 'Caucasian Females at Hospital #70',\n",
    "    ('age >= 80', '`ethnicity_Native American` == 1', '`hospital_id_100.0` == 1'): 'Native American Aged 80+ at Hospital #100',\n",
    "    ('gender_F == 1', '`ethnicity_African American` == 1', 'age >= 50 and age < 60'): 'African American Females Aged 50 to 59',\n",
    "    ('gender_M == 1', 'age >= 70 and age < 80', '`hospital_id_70.0` == 1'): 'Males Aged 70 to 79 at Hospital #70',\n",
    "    ('ethnicity_Caucasian == 1', 'age >= 80', '`hospital_id_118.0` == 1'): 'Caucasian Aged 80+ at Hospital #118',\n",
    "    ('age >= 60 and age < 70', '`ethnicity_Other/Unknown` == 1', '`hospital_id_70.0` == 1'): 'Unknown Ethnicity Aged 60 to 69 at Hospital #70',\n",
    "    ('gender_F == 1', 'age >= 70', '`ethnicity_Caucasian` == 1'): 'Caucasian Females Aged 70+',\n",
    "    ('age >= 80', 'gender_F == 1', '`hospital_id_118.0` == 1'): 'Females Aged 80+ at Hospital #118',\n",
    "  }\n",
    "\n",
    "  compare_performance_measures(X_test, y_test, xgboost, intersecting_3_subgroups_dict, model_name)\n",
    "\n",
    "  intersecting_4_subgroups_dict = {\n",
    "    'age > 0' : 'All data',\n",
    "    ('age >= 20 and age < 40', 'gender_M == 1', '`hospital_id_118.0` == 1', 'ethnicity_Caucasian == 1'): \"Caucasian Males Aged 20 to 39 at Hospital #118\",\n",
    "    ('age >= 60 and age < 70', 'gender_M == 1', '`hospital_id_118.0` == 0 and `hospital_id_19.0` == 0 and `hospital_id_188.0` == 0', '`ethnicity_Other/Unknown` == 1'): \"Other/Unknown Ethnicity Males, 60-69, Outside Top 3 Hospitals\",\n",
    "    ('age >= 50 and age < 60', 'gender_F == 1', '`hospital_id_118.0` == 1', '`ethnicity_African American` == 1'): \"African American Females Aged 50 to 59 at Hospital #161\",\n",
    "    ('age >= 20 and age < 50', 'gender_F == 1', '`hospital_id_19.0` == 1', 'ethnicity_Caucasian == 1'): \"Caucasian Females Aged 20 to 49 at Hospital #19\",\n",
    "  }\n",
    "\n",
    "  compare_performance_measures(X_test, y_test, xgboost, intersecting_4_subgroups_dict, model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_calibration_itl(model, X, y, subgroup_queries):\n",
    "  \"\"\"\n",
    "    Calculates the calibration-in-the-large (ITL) for each subgroup defined by the provided queries.\n",
    "\n",
    "    Calibration-in-the-large measures how well the average predicted probability for a subgroup\n",
    "    matches the observed event rate within that subgroup.\n",
    "\n",
    "    Args:\n",
    "        model: A fitted model with a `predict_proba` method.\n",
    "        X (pandas.DataFrame): The feature DataFrame.\n",
    "        y (pandas.Series): The target variable.\n",
    "        subgroup_queries (list): A list of queries defining the subgroups. Each query can be\n",
    "                                 a tuple of conditions or a single condition string.\n",
    "\n",
    "    Returns:\n",
    "        list:  A list of calibration-in-the-large values (floats), one for each subgroup query.\n",
    "  \"\"\"\n",
    "\n",
    "  y = pd.Series(y, index=X.index)\n",
    "  p = pd.Series(model.predict_proba(X)[:, 1], index=X.index)\n",
    "\n",
    "  calibrations_list = []\n",
    "\n",
    "  for query in subgroup_queries:\n",
    "    S = X.query(\" and \".join(query) if isinstance(query, tuple) else query).index\n",
    "    avg_predicted_proba = np.mean(p[S])\n",
    "    overall_event_rate = np.mean(y[S])\n",
    "    calibration_intercept = avg_predicted_proba - overall_event_rate\n",
    "    calibrations_list.append(calibration_intercept)\n",
    "\n",
    "  return calibrations_list\n",
    "\n",
    "\n",
    "def test_mc_across_models(model, sorted_queries, y_test, model_name):\n",
    "  \"\"\"\n",
    "  Evaluates the calibration-in-the-large (ITL) of a model across subgroups, both before and after calibration. Plots the results for comparison.\n",
    "\n",
    "  Args:\n",
    "      model: The fitted model to evaluate.\n",
    "      sorted_queries: A list of queries defining the subgroups, sorted by subgroup size.\n",
    "      y_test: The true labels (target values) for the test set.\n",
    "      model_name (str): The name of the model for the plot title.\n",
    "\n",
    "  Returns:\n",
    "      tuple:\n",
    "         - The calibrated model object (ClibratedPredictor)\n",
    "         - The original ITL calibration values\n",
    "         - The ITL calibration values after calibration\n",
    "  \"\"\"\n",
    "  if model_name == 'KNN' or model_name == 'LR':\n",
    "      calibrated_model = ClibratedPredictor(model, sorted_queries, y_test)\n",
    "      calibrations_list = calc_calibration_itl(model, X_test_scaled, y_test, sorted_queries) # 1.5 min. to run\n",
    "      calibrated_calibrations_list = calc_calibration_itl(calibrated_model, X_test_scaled, y_test, sorted_queries)\n",
    "  else:\n",
    "      calibrated_model = ClibratedPredictor(model, sorted_queries, y_test)\n",
    "      calibrations_list = calc_calibration_itl(model, X_test_xgb, y_test, sorted_queries) # 1.5 min. to run\n",
    "      calibrated_calibrations_list = calc_calibration_itl(calibrated_model, X_test_xgb, y_test, sorted_queries)\n",
    "\n",
    "  data = {\n",
    "    'Original': calibrations_list,\n",
    "    'Calibrated': calibrated_calibrations_list,\n",
    "    'Index': range(len(calibrations_list))\n",
    "  }\n",
    "\n",
    "  df = pd.DataFrame(data)\n",
    "\n",
    "  # Melt the DataFrame so that it's in a tidy format Plotly can use to automatically generate a legend\n",
    "  df_melted = df.melt(id_vars=['Index'], value_vars=['Original', 'Calibrated'],\n",
    "                      var_name='Type', value_name='Value')\n",
    "\n",
    "  # Plot using the melted DataFrame\n",
    "  fig = px.line(df_melted,\n",
    "                x='Index',\n",
    "                y='Value',\n",
    "                color='Type', # This will automatically create a legend based on the 'Type' column\n",
    "                title=f'Calibration in The Large Across Subgroups ({model_name} model)')\n",
    "\n",
    "  fig.update_layout(\n",
    "      xaxis_title=\"Subgroup Index (Widest to Narrowest)\",\n",
    "      yaxis_title=\"Calibration in The Large\"\n",
    "  )\n",
    "\n",
    "  fig.show()\n",
    "\n",
    "  return calibrated_model, calibrations_list, calibrated_calibrations_list"
   ],
   "id": "6e47dfbafffca88f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "calibrated_dict = {\n",
    "    'KNN': {'model': knn},\n",
    "    'LR': {'model': lr},\n",
    "    'XGB': {'model': xgboost}\n",
    "}\n",
    "\n",
    "for model_name, model_dict in calibrated_dict.items():\n",
    "  model_dict['calibrated-model'], model_dict['calibrations_list'], model_dict['calibrated-calibrations-list'] = \\\n",
    "  test_mc_across_models(model_dict['model'], sorted_queries, y_test, model_name)"
   ],
   "id": "15ac855d0e8df66b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_calibrated_metrics(model_name):\n",
    "  \"\"\"\n",
    "  Calculates and plots performance metrics for a calibrated model.\n",
    "\n",
    "  Args:\n",
    "      model_name (str): The name of the model to retrieve from the 'calibrated_dict'.\n",
    "\n",
    "  Explanation:\n",
    "      1. Retrieves the calibrated model and predictions from the 'calibrated_dict'.\n",
    "      2. Calculates performance metrics (AUC, Sensitivity, PPV, Specificity, NPV).\n",
    "      3. Creates a DataFrame to store the calculated metric values.\n",
    "      4. Generates a bar chart using Plotly to visualize the performance metrics.\n",
    "  \"\"\"\n",
    "\n",
    "  predictions = calibrated_dict[model_name]['calibrated-model'].predict(X_test)\n",
    "  probas = calibrated_dict[model_name]['calibrated-model'].proba[:,1]\n",
    "  metrics = ['Sensitivity', 'PPV', 'Specificity', 'NPV']\n",
    "\n",
    "  metrics_dict = {k: None for k in metrics}\n",
    "\n",
    "  tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "\n",
    "  metrics_dict['AUC'] = roc_auc_score(y_test, probas)\n",
    "  metrics_dict['Sensitivity'] = tp / (tp + fn)\n",
    "  metrics_dict['PPV'] = tp / (tp + fp)\n",
    "  metrics_dict['Specificity'] = tn / (tn + fp)\n",
    "  metrics_dict['NPV'] = tn / (tn + fn) if (tn + fn) != 0 else None\n",
    "\n",
    "  metrics_df = pd.DataFrame.from_dict(metrics_dict, orient='index', columns=['Value'])\n",
    "  metrics_df.reset_index(inplace=True)\n",
    "  metrics_df.rename(columns={'index': 'Metric'}, inplace=True)\n",
    "\n",
    "  fig = px.bar(metrics_df, x='Metric', y='Value', title=f'Model Calibrated Performance Metrics on {model_name}')\n",
    "\n",
    "  fig.update_layout(\n",
    "      xaxis_title=None,\n",
    "      yaxis_title=\"Metric Value\",\n",
    "  )\n",
    "\n",
    "  fig.show()\n",
    "\n",
    "\n",
    "def plot_regular_metrics(model_name):\n",
    "  \"\"\"\n",
    "    Calculates and plots performance metrics for the original (uncalibrated) model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to retrieve from the 'calibrated_dict'.\n",
    "\n",
    "    Differences from 'plot_calibrated_metrics':\n",
    "        *  Retrieves the original, uncalibrated model from 'calibrated_dict'.\n",
    "        *  The title indicates that these are performance metrics for the original model.\n",
    "\n",
    "    Explanation:\n",
    "        1. Retrieves the original model and predictions from the 'calibrated_dict'.\n",
    "        2. Calculates performance metrics (AUC, Sensitivity, PPV, Specificity, NPV).\n",
    "        3. Creates a DataFrame to store the calculated metric values.\n",
    "        4. Generates a bar chart using Plotly to visualize the performance metrics.\n",
    "  \"\"\"\n",
    "  predictions = calibrated_dict[model_name]['model'].predict(X_test)\n",
    "  probas = calibrated_dict[model_name]['model'].predict_proba(X_test)[:,1]\n",
    "  metrics = ['Sensitivity', 'PPV', 'Specificity', 'NPV']\n",
    "\n",
    "  metrics_dict = {k: None for k in metrics}\n",
    "\n",
    "  tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "\n",
    "  metrics_dict['AUC'] = roc_auc_score(y_test, probas)\n",
    "  metrics_dict['Sensitivity'] = tp / (tp + fn)\n",
    "  metrics_dict['PPV'] = tp / (tp + fp)\n",
    "  metrics_dict['Specificity'] = tn / (tn + fp)\n",
    "  metrics_dict['NPV'] = tn / (tn + fn) if (tn + fn) != 0 else None\n",
    "\n",
    "  metrics_df = pd.DataFrame.from_dict(metrics_dict, orient='index', columns=['Value'])\n",
    "  metrics_df.reset_index(inplace=True)\n",
    "  metrics_df.rename(columns={'index': 'Metric'}, inplace=True)\n",
    "\n",
    "  fig = px.bar(metrics_df, x='Metric', y='Value', title=f'Model Performance Metrics on {model_name}')\n",
    "  fig.update_traces(marker_color='#960b0b')  # Your desired color\n",
    "\n",
    "  fig.update_layout(\n",
    "      xaxis_title=None,\n",
    "      yaxis_title=\"Metric Value\",\n",
    "  )\n",
    "\n",
    "  fig.show()\n",
    "\n",
    "\n",
    "def plot_distribution(model_name):\n",
    "  \"\"\"\n",
    "    Visualizes the distribution of calibration-in-the-large (ITL) values before and after applying a multi-calibration process.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to retrieve data from the 'calibrated_dict'.\n",
    "\n",
    "    Explanation:\n",
    "        1. Retrieves calibration-in-the-large values (both before and after calibration) from the 'calibrated_dict'.\n",
    "        2. Creates a DataFrame to organize the calibration data.\n",
    "        3. Generates an overlaid histogram using Plotly to compare the distributions.\n",
    "        4. Sets the y-axis to a logarithmic scale for better visualization of distributions.\n",
    "  \"\"\"\n",
    "  vector1 = calibrated_dict[model_name]['calibrations_list']\n",
    "  vector2 = calibrated_dict[model_name]['calibrated-calibrations-list']\n",
    "  df = pd.DataFrame({'Data 1': vector1, 'Data 2': vector2})\n",
    "  fig = px.histogram(df, x=df.columns, barmode='overlay', opacity=0.6,\n",
    "                   title=f'Distribution of Calibration on {model_name} Before and After Multi-Calibration')\n",
    "\n",
    "  fig.update_layout(bargap=0.1, yaxis_type='log')  # Set y-axis to log scale\n",
    "\n",
    "  fig.show()"
   ],
   "id": "cd3fbb4d819482c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the data\n",
    "gender_counts = df['gender'].value_counts()\n",
    "ethnicity_counts = df['ethnicity'].value_counts()\n",
    "age_data = df['age']\n",
    "\n",
    "# Create a subplot figure with 1 row and 3 columns\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"histogram\"}]],\n",
    "    subplot_titles=(\"Gender Distribution\", \"Ethnicity Distribution\", \"Age Distribution\"),\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Add the plots\n",
    "fig.add_trace(go.Bar(x=gender_counts.index, y=gender_counts.values, marker_color='green'), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=ethnicity_counts.index, y=ethnicity_counts.values, marker_color='orange'), row=1, col=2)\n",
    "fig.add_trace(go.Histogram(x=age_data, nbinsx=20, marker_color='red'), row=1, col=3)\n",
    "\n",
    "# Update layout for a better view\n",
    "fig.update_layout(showlegend=False, height=400, width=1200)\n",
    "fig.update_xaxes(title_text=\"Gender\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Ethnicity\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Age\", row=1, col=3)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=3)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ],
   "id": "bc6b82e5de815d90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Multi - Calibration",
   "id": "5d0958fa445c39e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_subgroups(query_lists):\n",
    "    \"\"\"Generate all possible combinations of subgroups from different categorical lists\"\"\"\n",
    "    subgroups = []\n",
    "\n",
    "    # Generate all combinations\n",
    "    for r in range(2, len(query_lists) + 1):\n",
    "        for combination in product(*[query_lists[i] for i in range(len(query_lists))]):\n",
    "            subgroups.append(combination[:r])\n",
    "\n",
    "    # Also add pairwise combinations\n",
    "    for i in range(len(query_lists)):\n",
    "        for j in range(i + 1, len(query_lists)):\n",
    "            for q1 in query_lists[i]:\n",
    "                for q2 in query_lists[j]:\n",
    "                    subgroups.append((q1, q2))\n",
    "\n",
    "    return subgroups"
   ],
   "id": "a33154be44aecd7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_empty_subgroups(subpopulation_queries, X):\n",
    "    \"\"\"Find subgroups that have no data points\"\"\"\n",
    "    empty_subgroups = []\n",
    "\n",
    "    for query in subpopulation_queries:\n",
    "        try:\n",
    "            if isinstance(query, tuple):\n",
    "                combined_query = \" and \".join(query)\n",
    "            else:\n",
    "                combined_query = query\n",
    "\n",
    "            subset = X.query(combined_query)\n",
    "            if len(subset) == 0:\n",
    "                empty_subgroups.append(query)\n",
    "        except Exception as e:\n",
    "            # If query fails, consider it empty\n",
    "            empty_subgroups.append(query)\n",
    "            print(f\"Query failed: {query}, Error: {e}\")\n",
    "\n",
    "    return empty_subgroups"
   ],
   "id": "3eb821f7a5f39b68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define subgroups (removed hospital queries)\n",
    "gender_queries = ['gender_M == 1', 'gender_F == 1']\n",
    "age_queries = [f'age >= {a} and age < {a+10}' for a in range(10, 90, 10)]\n",
    "ethnicity_queries = [f'`ethnicity_{e}` == 1' for e in ['Caucasian', 'Native American', 'Other/Unknown', 'African American', 'Asian']]\n",
    "\n",
    "# Generate all subgroups without hospital\n",
    "all_query_lists = [gender_queries, age_queries, ethnicity_queries]\n",
    "all_subgroups = generate_subgroups(all_query_lists)\n",
    "\n",
    "subpopulation_queries = [\n",
    "    *gender_queries,\n",
    "    *age_queries,\n",
    "    *ethnicity_queries,\n",
    "    *all_subgroups\n",
    "]\n",
    "\n",
    "print(f'Total subgroups (without hospital) = {len(subpopulation_queries)}')"
   ],
   "id": "5c5243f5d9c132ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CalibratedPredictor:\n",
    "    def __init__(self, model, scaler, subgroups, y_test):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.subgroups = subgroups\n",
    "        self.y_test = y_test\n",
    "        self.proba = None\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        if self.proba is None:\n",
    "            self.predict_proba(X, y)\n",
    "\n",
    "        p = self.proba[:, 1]\n",
    "        preds = np.where(p >= 0.2, 1, 0)\n",
    "        return preds\n",
    "\n",
    "    def predict_proba(self, X, y=None):\n",
    "        return self.multi_calibrate_predictor(X, y, self.subgroups)\n",
    "\n",
    "    def multi_calibrate_predictor(self, X, y, subpopulation_queries, alpha=1e-3, max_iter=1):\n",
    "        \"\"\"\n",
    "        Perform multi-calibration to ensure fairness across subgroups.\n",
    "\n",
    "        :param X: DataFrame containing the input features.\n",
    "        :param y: Series containing the true labels.\n",
    "        :param subpopulation_queries: List of strings representing the query to define each subgroup.\n",
    "        :param alpha: The violation parameter.\n",
    "        :param max_iter: Maximum number of iterations for the calibration process.\n",
    "        :return: Calibrated probabilities.\n",
    "        \"\"\"\n",
    "\n",
    "        y = pd.Series(self.y_test, index=X.index)\n",
    "\n",
    "        # Apply scaling if scaler exists\n",
    "        if self.scaler is not None:\n",
    "            X_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X),\n",
    "                columns=X.columns,\n",
    "                index=X.index\n",
    "            )\n",
    "        else:\n",
    "            X_scaled = X\n",
    "\n",
    "        # Get initial predictions\n",
    "        p = self.model.predict_proba(X_scaled)[:, 1]\n",
    "        calibrated_p = pd.Series(p.copy(), index=X.index)\n",
    "        done = False\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "            done = True\n",
    "            violations_found = 0\n",
    "\n",
    "            for query in subpopulation_queries:\n",
    "                try:\n",
    "                    if isinstance(query, tuple):\n",
    "                        combined_query = \" and \".join(query)\n",
    "                    else:\n",
    "                        combined_query = query\n",
    "\n",
    "                    S = X.query(combined_query).index\n",
    "\n",
    "                    if len(S) == 0:  # Skip empty subgroups\n",
    "                        continue\n",
    "\n",
    "                    delta_S = (y.loc[S] - calibrated_p.loc[S]).mean()\n",
    "                    if abs(delta_S) > alpha:\n",
    "                        calibrated_p.loc[S] += delta_S\n",
    "                        done = False\n",
    "                        violations_found += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing query {query}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"Iteration {iteration + 1}: {violations_found} violations found\")\n",
    "            if done:\n",
    "                print(\"Calibration completed!\")\n",
    "                break\n",
    "\n",
    "        # Ensure probabilities are in [0, 1]\n",
    "        calibrated_p = np.clip(calibrated_p.values, 0, 1)\n",
    "\n",
    "        # Create probability matrix for binary classification\n",
    "        zero_class = 1 - calibrated_p\n",
    "        calibrated_p_matrix = np.column_stack([zero_class, calibrated_p])\n",
    "\n",
    "        self.proba = calibrated_p_matrix\n",
    "        return calibrated_p_matrix\n"
   ],
   "id": "ca49d7b8ca0a99a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_mc_across_models(model, scaler, sorted_queries, y_test, model_name):\n",
    "    \"\"\"Test multi-calibration across models\"\"\"\n",
    "    print(f\"\\n=== Testing Multi-Calibration for {model_name} ===\")\n",
    "\n",
    "    # Create calibrated predictor\n",
    "    calibrated_model = CalibratedPredictor(model, scaler, sorted_queries, y_test)\n",
    "\n",
    "    # You can add more evaluation logic here\n",
    "    # For now, just return the calibrated model\n",
    "    return calibrated_model, sorted_queries, sorted_queries"
   ],
   "id": "a41b57ef81362a27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_multi_calibration(X_test, y_test, models_path=\"/Users/ormeiri/Desktop/predictive_models_git/models/final_models\"):\n",
    "    \"\"\"Main function to run multi-calibration on all models\"\"\"\n",
    "\n",
    "    # Load models\n",
    "    models_dict = load_models_and_scalers(models_path)\n",
    "\n",
    "    # Find empty subgroups\n",
    "    print(\"Finding empty subgroups...\")\n",
    "    empty_subgroups = find_empty_subgroups(subpopulation_queries, X_test)\n",
    "    final_subgroup_queries = set(subpopulation_queries) - set(empty_subgroups)\n",
    "\n",
    "    sorted_queries = sorted(final_subgroup_queries, key=lambda x: 0 if isinstance(x, str) else len(x))\n",
    "    print(f'Valid subgroups = {len(sorted_queries)}')\n",
    "\n",
    "    # Create calibrated models dictionary\n",
    "    calibrated_dict = {}\n",
    "\n",
    "    for model_name, model_data in models_dict.items():\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "\n",
    "        calibrated_model, calibrations_list, calibrated_calibrations_list = test_mc_across_models(\n",
    "            model_data['model'],\n",
    "            model_data['scaler'],\n",
    "            sorted_queries,\n",
    "            y_test,\n",
    "            model_name\n",
    "        )\n",
    "\n",
    "        calibrated_dict[model_name] = {\n",
    "            'original_model': model_data['model'],\n",
    "            'scaler': model_data['scaler'],\n",
    "            'calibrated_model': calibrated_model,\n",
    "            'calibrations_list': calibrations_list,\n",
    "            'calibrated_calibrations_list': calibrated_calibrations_list,\n",
    "            'metadata': model_data['metadata']\n",
    "        }\n",
    "\n",
    "    return calibrated_dict"
   ],
   "id": "5e632363f9ca8074",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "calibrated_models = run_multi_calibration(X_test, y_test)",
   "id": "1aa8b174c384f0b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "932a2d3933a23117",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
